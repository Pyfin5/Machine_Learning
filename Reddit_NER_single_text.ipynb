{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqVIv/ZnuLp0TdreaelbA3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pyfin5/Machine_Learning/blob/main/Reddit_NER_single_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qE3wja6QUd6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "375a15b7-4bad-4a57-c670-e7220f64d5a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab-Notebooks/Reddit_listening/"
      ],
      "metadata": {
        "id": "y3hcz7ewVE44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "098dd34f-ed7e-4007-e06e-5f323344cfe6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab-Notebooks/Reddit_listening\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip"
      ],
      "metadata": {
        "id": "9RQYI5Q9VI5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "010bf598-4237-475e-c1aa-b1a13c25f428"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \\\n",
        "    praw \\\n",
        "    pandas \\\n",
        "    numpy \\\n",
        "    python-dotenv \\\n",
        "    datetime \\\n",
        "    IPython \\\n",
        "    llama_index \\\n",
        "    langchain_google_vertexai \\\n",
        "    google-cloud-aiplatform \\\n",
        "    praw \\\n",
        "    textwrap3 \\\n",
        "    langextract \\\n",
        "    asyncpraw"
      ],
      "metadata": {
        "id": "hjotf5luVL_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7e818e4-a5df-4c5f-9415-b8e07fbbd123"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting datetime\n",
            "  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n",
            "Collecting llama_index\n",
            "  Downloading llama_index-0.13.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting langchain_google_vertexai\n",
            "  Downloading langchain_google_vertexai-2.0.28-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.11/dist-packages (1.109.0)\n",
            "Collecting textwrap3\n",
            "  Downloading textwrap3-0.9.2-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langextract\n",
            "  Downloading langextract-1.0.8-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting asyncpraw\n",
            "  Downloading asyncpraw-7.8.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Collecting zope.interface (from datetime)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from IPython) (75.2.0)\n",
            "Collecting jedi>=0.16 (from IPython)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from IPython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from IPython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from IPython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from IPython) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from IPython) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from IPython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from IPython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from IPython) (4.9.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython) (0.2.13)\n",
            "Collecting llama-index-cli<0.6,>=0.5.0 (from llama_index)\n",
            "  Downloading llama_index_cli-0.5.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting llama-index-core<0.14,>=0.13.2 (from llama_index)\n",
            "  Downloading llama_index_core-0.13.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.6,>=0.5.0 (from llama_index)\n",
            "  Downloading llama_index_embeddings_openai-0.5.0-py3-none-any.whl.metadata (400 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.9.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting llama-index-llms-openai<0.6,>=0.5.0 (from llama_index)\n",
            "  Downloading llama_index_llms_openai-0.5.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting llama-index-readers-file<0.6,>=0.5.0 (from llama_index)\n",
            "  Downloading llama_index_readers_file-0.5.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_readers_llama_parse-0.5.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama_index) (3.9.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.2->llama_index) (3.12.15)\n",
            "Collecting aiosqlite (from llama-index-core<0.14,>=0.13.2->llama_index)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting banks<3,>=2.2.0 (from llama-index-core<0.14,>=0.13.2->llama_index)\n",
            "  Downloading banks-2.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dataclasses-json (from llama-index-core<0.14,>=0.13.2->llama_index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.14,>=0.13.2->llama_index)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.14,>=0.13.2->llama_index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.14,>=0.13.2->llama_index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.2->llama_index) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.2->llama_index) (0.28.1)\n",
            "Collecting llama-index-workflows<2,>=1.0.1 (from llama-index-core<0.14,>=0.13.2->llama_index)\n",
            "  Downloading llama_index_workflows-1.3.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.2->llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.2->llama_index) (3.5)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.2->llama_index) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.2->llama_index) (4.3.8)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.2->llama_index) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.2->llama_index) (6.0.2)\n",
            "Collecting setuptools>=18.5 (from IPython)\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.2->llama_index) (2.0.43)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.2->llama_index) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.2->llama_index) (0.11.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.2->llama_index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.2->llama_index) (4.14.1)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.14,>=0.13.2->llama_index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.2->llama_index) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.2->llama_index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.2->llama_index) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.2->llama_index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.2->llama_index) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.2->llama_index) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.2->llama_index) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.2->llama_index) (1.20.1)\n",
            "Collecting griffe (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.2->llama_index)\n",
            "  Downloading griffe-1.12.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.2->llama_index) (3.1.6)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama_index) (1.99.9)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama_index) (4.13.4)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama_index) (0.7.1)\n",
            "Collecting pypdf<7,>=5.1.0 (from llama-index-readers-file<0.6,>=0.5.0->llama_index)\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.6,>=0.5.0->llama_index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama_index) (2.7)\n",
            "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.2->llama_index)\n",
            "  Downloading llama_index_instrumentation-0.4.0-py3-none-any.whl.metadata (252 bytes)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama_index) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama_index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama_index) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama_index) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.14,>=0.13.2->llama_index) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.2->llama_index) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.2->llama_index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.2->llama_index) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.2->llama_index) (0.4.1)\n",
            "Requirement already satisfied: bottleneck<2.0.0,>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai) (1.4.2)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai) (2.19.0)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_google_vertexai)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.67 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai) (0.3.74)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.6 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai) (2.11.0)\n",
            "Collecting pyarrow<20.0.0,>=19.0.1 (from langchain_google_vertexai)\n",
            "  Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting validators<1,>=0.22.0 (from langchain_google_vertexai)\n",
            "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.25.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (5.29.5)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (25.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (3.35.1)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (1.14.2)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (2.1.1)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (1.30.0)\n",
            "Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (0.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (4.9.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform) (0.14.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai) (1.7.1)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (15.0.1)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.67->langchain_google_vertexai) (0.4.14)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.67->langchain_google_vertexai) (1.33)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.67->langchain_google_vertexai) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langextract) (1.4.0)\n",
            "Collecting async_timeout>=4.0.0 (from langextract)\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting exceptiongroup>=1.1.0 (from langextract)\n",
            "  Downloading exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting ml-collections>=0.1.0 (from langextract)\n",
            "  Downloading ml_collections-1.1.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: more-itertools>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from langextract) (10.7.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.11/dist-packages (from asyncpraw) (24.1.0)\n",
            "Collecting aiosqlite (from llama-index-core<0.14,>=0.13.2->llama_index)\n",
            "  Downloading aiosqlite-0.17.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting asyncprawcore<3,>=2.4 (from asyncpraw)\n",
            "  Downloading asyncprawcore-2.4.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->IPython) (0.8.4)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.3.67->langchain_google_vertexai) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.3.67->langchain_google_vertexai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.3.67->langchain_google_vertexai) (0.23.0)\n",
            "Collecting llama-cloud==0.1.35 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud-0.1.35-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_parse-0.6.60-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.60 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud_services-0.6.60-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: click<9,>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.60->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index) (8.2.1)\n",
            "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_parse-0.6.59-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.59 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud_services-0.6.59-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_parse-0.6.58-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.58 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud_services-0.6.58-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_parse-0.6.57-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-cloud-services>=0.6.56 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud_services-0.6.57-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading llama_cloud_services-0.6.56-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_parse-0.6.56-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading llama_parse-0.6.55-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.55 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud_services-0.6.55-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_parse-0.6.54-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.54 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud_services-0.6.54-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (2024.11.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->IPython) (0.7.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.2->llama_index) (3.2.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.2->llama_index)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.14,>=0.13.2->llama_index)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.2->llama_index)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.2->llama_index) (3.0.2)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
            "Downloading llama_index-0.13.2-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_cli-0.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.13.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading banks-2.2.0-py3-none-any.whl (29 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_index_embeddings_openai-0.5.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_llms_openai-0.5.4-py3-none-any.whl (25 kB)\n",
            "Downloading llama_index_readers_file-0.5.2-py3-none-any.whl (51 kB)\n",
            "Downloading llama_index_workflows-1.3.0-py3-none-any.whl (42 kB)\n",
            "Downloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading langchain_google_vertexai-2.0.28-py3-none-any.whl (103 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
            "Downloading textwrap3-0.9.2-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langextract-1.0.8-py3-none-any.whl (84 kB)\n",
            "Downloading asyncpraw-7.8.1-py3-none-any.whl (196 kB)\n",
            "Downloading aiosqlite-0.17.0-py3-none-any.whl (15 kB)\n",
            "Downloading asyncprawcore-2.4.0-py3-none-any.whl (19 kB)\n",
            "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_indices_managed_llama_cloud-0.9.2-py3-none-any.whl (16 kB)\n",
            "Downloading llama_cloud-0.1.35-py3-none-any.whl (303 kB)\n",
            "Downloading llama_index_instrumentation-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.5.0-py3-none-any.whl (3.2 kB)\n",
            "Downloading llama_parse-0.6.54-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_cloud_services-0.6.54-py3-none-any.whl (63 kB)\n",
            "Downloading ml_collections-1.1.0-py3-none-any.whl (76 kB)\n",
            "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading griffe-1.12.1-py3-none-any.whl (138 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "Installing collected packages: textwrap3, striprtf, filetype, dirtyjson, validators, setuptools, python-dotenv, pypdf, pyarrow, mypy-extensions, ml-collections, marshmallow, jedi, httpx-sse, exceptiongroup, deprecated, colorama, async_timeout, aiosqlite, zope.interface, update_checker, typing-inspect, prawcore, griffe, praw, llama-index-instrumentation, llama-cloud, datetime, dataclasses-json, banks, asyncprawcore, llama-index-workflows, langextract, asyncpraw, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-cli, langchain_google_vertexai, llama-index-readers-llama-parse, llama_index\n",
            "\u001b[2K  Attempting uninstall: setuptools\n",
            "\u001b[2K    Found existing installation: setuptools 75.2.0\n",
            "\u001b[2K    Uninstalling setuptools-75.2.0:\n",
            "\u001b[2K      Successfully uninstalled setuptools-75.2.0\n",
            "\u001b[2K  Attempting uninstall: pyarrow\n",
            "\u001b[2K    Found existing installation: pyarrow 18.1.0\n",
            "\u001b[2K    Uninstalling pyarrow-18.1.0:\n",
            "\u001b[2K      Successfully uninstalled pyarrow-18.1.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45/45\u001b[0m [llama_index]\n",
            "\u001b[1A\u001b[2KSuccessfully installed aiosqlite-0.17.0 async_timeout-5.0.1 asyncpraw-7.8.1 asyncprawcore-2.4.0 banks-2.2.0 colorama-0.4.6 dataclasses-json-0.6.7 datetime-5.5 deprecated-1.2.18 dirtyjson-1.0.8 exceptiongroup-1.3.0 filetype-1.2.0 griffe-1.12.1 httpx-sse-0.4.1 jedi-0.19.2 langchain_google_vertexai-2.0.28 langextract-1.0.8 llama-cloud-0.1.35 llama-cloud-services-0.6.54 llama-index-cli-0.5.0 llama-index-core-0.13.2 llama-index-embeddings-openai-0.5.0 llama-index-indices-managed-llama-cloud-0.9.2 llama-index-instrumentation-0.4.0 llama-index-llms-openai-0.5.4 llama-index-readers-file-0.5.2 llama-index-readers-llama-parse-0.5.0 llama-index-workflows-1.3.0 llama-parse-0.6.54 llama_index-0.13.2 marshmallow-3.26.1 ml-collections-1.1.0 mypy-extensions-1.1.0 praw-7.8.1 prawcore-2.4.0 pyarrow-19.0.1 pypdf-6.0.0 python-dotenv-1.1.1 setuptools-80.9.0 striprtf-0.0.26 textwrap3-0.9.2 typing-inspect-0.9.0 update_checker-0.18.0 validators-0.35.0 zope.interface-7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "import datetime as dt\n",
        "import pandas as pd  # Data manipulation library\n",
        "import sys\n",
        "from IPython.display import Markdown, display\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from google.cloud import aiplatform\n",
        "from praw.models import MoreComments\n",
        "import textwrap\n",
        "import langextract as lx\n",
        "from google.colab import userdata\n",
        "import asyncpraw as asyncpraw"
      ],
      "metadata": {
        "id": "e2Y2R1Q_VOjI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv() # Load environment variables from .env file\n",
        "\n",
        "reddit = praw.Reddit(client_id=userdata.get(\"client_id\"),\n",
        "                     client_secret=userdata.get(\"client_secret\"),\n",
        "                     user_agent=userdata.get(\"user_agent\"),\n",
        "                     username=userdata.get(\"username\"),\n",
        "                     password=userdata.get(\"password\"))\n",
        "\n",
        "\n",
        "# Subreddit to scrape\n",
        "subreddit = reddit.subreddit('Learnprogramming')\n",
        "\n",
        "# Display the name of the Subreddit\n",
        "print(\"Display Name:\", subreddit.display_name)"
      ],
      "metadata": {
        "id": "XOH6MjoOVXHl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4fd0003-84c4-4fd0-d721-2d3ec8532182"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Display Name: Learnprogramming\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission = reddit.submission(url = \"https://www.reddit.com/r/learnprogramming/comments/vcpqq6/whats_up_with_linux_and_software_developers_if_i/\")\n"
      ],
      "metadata": {
        "id": "qd1xyc9T-eUZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(submission)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFLyTTlZDl4X",
        "outputId": "837c488d-9418-4525-d323-0726a30e06f3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vcpqq6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure to extract all operating systems mentioned in the post. Each post might contain mention of multiple operating systems, extract them all.\n",
        "# if you miss an extraction, you will be spanked.\n",
        "\n",
        "system_prompt = \"Extract operating system mentioned by the post author. Make sure to provide all instances of operating systems mentioned in the post.\"  \\\n",
        "\"Each post might contain mention of multiple operating systems, extract them all including multple instances of the same operating system.\"\n",
        "\n",
        "examples = [\n",
        "            lx.data.ExampleData(\n",
        "                text=   \"I've used Linux for 18 years. For me, Linux is cleaner, easier to install, maintain and update, and is just faster — and there are no ads popping up.\" \\\n",
        "                        \"That said, I don't play video games and I don't know the current state of video games on Linux. In the past, when people asked me if Linux was for them, I would ask two questions...\" \\\n",
        "                        \"As for programming (which I don't do) I think Linux would be a great platform for application development.\"\n",
        "                        ,\n",
        "                extractions=[\n",
        "                    lx.data.Extraction(extraction_class=\"distribution\", extraction_text=\"Linux\", attributes = {'User':'Yes'})\n",
        "                ]\n",
        "            ),\n",
        "            lx.data.ExampleData(\n",
        "                text=   \"I've always been a Windows user. I bought and used a windows for the first time professionally a few months ago \" \\\n",
        "                \"Learn to program on whatever you're familiar with. It's just easier.\",\n",
        "                extractions=[\n",
        "                    lx.data.Extraction(extraction_class=\"distribution\", extraction_text=\"Windows\", attributes = {'User':'Yes'})\n",
        "                ],\n",
        "            ),\n",
        "            lx.data.ExampleData( #New example which is not a user.\n",
        "                text=   \"Also, don't forget that running a Windows server is expensive. Linux is free.\" \\\n",
        "                \"And you probably want to develop on the same or similar OS to what you'll be running on production. \",\n",
        "                extractions=[\n",
        "                    lx.data.Extraction(extraction_class=\"distribution\", extraction_text=\"Windows\", attributes = {'User':'No'})\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "\n"
      ],
      "metadata": {
        "id": "L9W6AfRBViMq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entity(input_text, NER_examples, system_prompt, text_name):\n",
        "\n",
        "        # Define example data with entities in order of appearance\n",
        "        examples = NER_examples\n",
        "\n",
        "        result = lx.extract(\n",
        "            text_or_documents=input_text,\n",
        "            prompt_description=system_prompt,\n",
        "            examples=examples,\n",
        "            model_id=\"gemini-2.5-flash\",\n",
        "            api_key=userdata.get('GOOGLE_AI_STUDIO_API_KEY')\n",
        "        )\n",
        "\n",
        "        # Display entities with positions\n",
        "        print(f\"Input: {input_text}\\n\")\n",
        "        print(\"Extracted entities:\")\n",
        "        for entity in result.extractions:\n",
        "            position_info = \"\"\n",
        "            if entity.char_interval:\n",
        "                start, end = entity.char_interval.start_pos, entity.char_interval.end_pos\n",
        "                position_info = f\" (pos: {start}-{end})\"\n",
        "            print(f\"• {entity.extraction_class.capitalize()}: {entity.extraction_text}{position_info}\")\n",
        "            print(f\"  Attributes: {entity.attributes}\") #Add print statement\n",
        "\n",
        "\n",
        "        print(result)\n",
        "        # Save and visualize the results\n",
        "        lx.io.save_annotated_documents([result], output_name=\"{}.jsonl\".format(text_name), output_dir=\".\")\n",
        "\n",
        "        # Generate the interactive visualization\n",
        "        html_content = lx.visualize(\"{}.jsonl\".format(text_name))\n",
        "        with open(\"{}.html\".format(text_name), \"w\") as f:\n",
        "          if hasattr(html_content, 'data'):\n",
        "            f.write(html_content.data)\n",
        "          else:\n",
        "            f.write(html_content)\n",
        "\n",
        "        print(\"Interactive visualization saved to {}visualization.html\".format(text_name))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BAxWVhCBVmzb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Getting the same up and running (and stable!) in windows is a lot of work, especially if you want it to be smooth.. Linux is more configurable, and it is what most of the world's infrastructure and systems run on.\" \\\n",
        "\"And yes, Linux is generally much more efficient than Windows.\" \\\n",
        "\"That said, these days it is mainly preference.\" \\\n",
        "\"For example, I prefer Linux so that I can configure it exactly how I want.\" \\\n",
        "\"I like to use I3wm (A tiling wm) with a custom set of commands combined with oh-my-fish with my custom dotfiles.\"\\\n",
        "\"I have also used MacOS but only minimally, liked it when I used it for a bit.\""
      ],
      "metadata": {
        "id": "7GOOc7xU_u-M"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dist_extract = extract_entity(system_prompt=system_prompt, NER_examples = examples, input_text = input_text, text_name = 'linux_extract')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6StIFf1w_ppH",
        "outputId": "d1947655-f326-4398-b0ec-90150067f32d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-18 23:43:36,853 - langextract.debug - DEBUG - [langextract.inference] CALL: BaseLanguageModel.__init__(self=<GeminiLanguageModel>, constraint=Constraint(co...NONE: 'none'>), kwargs={})\n",
            "2025-08-18 23:43:36,854 - langextract.debug - DEBUG - [langextract.inference] RETURN: BaseLanguageModel.__init__ -> None (0.0 ms)\n",
            "2025-08-18 23:43:36,855 - langextract.debug - DEBUG - [langextract.inference] CALL: BaseLanguageModel.apply_schema(self=<GeminiLanguageModel>, schema_instance=GeminiSchema(...xtractions']}))\n",
            "2025-08-18 23:43:36,856 - langextract.debug - DEBUG - [langextract.inference] RETURN: BaseLanguageModel.apply_schema -> None (0.0 ms)\n",
            "DEBUG:absl:Initialized Annotator with prompt:\n",
            "Extract operating system mentioned by the post author. Make sure to provide all instances of operating systems mentioned in the post.Each post might contain mention of multiple operating systems, extract them all including multple instances of the same operating system.\n",
            "\n",
            "Examples\n",
            "Q: I've used Linux for 18 years. For me, Linux is cleaner, easier to install, maintain and update, and is just faster — and there are no ads popping up.That said, I don't play video games and I don't know the current state of video games on Linux. In the past, when people asked me if Linux was for them, I would ask two questions...As for programming (which I don't do) I think Linux would be a great platform for application development.\n",
            "A: {\n",
            "  \"extractions\": [\n",
            "    {\n",
            "      \"distribution\": \"Linux\",\n",
            "      \"distribution_attributes\": {\n",
            "        \"User\": \"Yes\"\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "Q: I've always been a Windows user. I bought and used a windows for the first time professionally a few months ago Learn to program on whatever you're familiar with. It's just easier.\n",
            "A: {\n",
            "  \"extractions\": [\n",
            "    {\n",
            "      \"distribution\": \"Windows\",\n",
            "      \"distribution_attributes\": {\n",
            "        \"User\": \"Yes\"\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "Q: Also, don't forget that running a Windows server is expensive. Linux is free.And you probably want to develop on the same or similar OS to what you'll be running on production. \n",
            "A: {\n",
            "  \"extractions\": [\n",
            "    {\n",
            "      \"distribution\": \"Windows\",\n",
            "      \"distribution_attributes\": {\n",
            "        \"User\": \"No\"\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "Q: \n",
            "A: \n",
            "INFO:absl:Starting document annotation.\n",
            "\u001b[94m\u001b[1mLangExtract\u001b[0m: model=\u001b[92mgemini-2.5-flash\u001b[0m [00:00]2025-08-18 23:43:36,862 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text=\"Getting the same up and running (and stable!) in windows is a lot of work, especially if you want it to be smooth.. Linux is more configurable, and it is what most of the world's infrastructure and systems run on.And yes, Linux is generally much m...prefer Linux so that I can configure it exactly how I want.I like to use I3wm (A tiling wm) with a custom set of commands combined with oh-my-fish with my custom dotfiles.I have also used MacOS but only minimally, liked it when I used it for a bit.\")\n",
            "2025-08-18 23:43:36,868 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (4.6 ms)\n",
            "INFO:absl:Processing batch 0 with length 1\n",
            "DEBUG:absl:Token util returns string: Getting the same up and running (and stable!) in windows is a lot of work, especially if you want it to be smooth.. Linux is more configurable, and it is what most of the world's infrastructure and systems run on.And yes, Linux is generally much more efficient than Windows.That said, these days it is mainly preference.For example, I prefer Linux so that I can configure it exactly how I want.I like to use I3wm (A tiling wm) with a custom set of commands combined with oh-my-fish with my custom dotfiles.I have also used MacOS but only minimally, liked it when I used it for a bit. for tokenized_text: TokenizedText(text=\"Getting the same up and running (and stable!) in windows is a lot of work, especially if you want it to be smooth.. Linux is more configurable, and it is what most of the world's infrastructure and systems run on.And yes, Linux is generally much more efficient than Windows.That said, these days it is mainly preference.For example, I prefer Linux so that I can configure it exactly how I want.I like to use I3wm (A tiling wm) with a custom set of commands combined with oh-my-fish with my custom dotfiles.I have also used MacOS but only minimally, liked it when I used it for a bit.\", tokens=[Token(index=0, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=0, end_pos=7), first_token_after_newline=False), Token(index=1, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=8, end_pos=11), first_token_after_newline=False), Token(index=2, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=12, end_pos=16), first_token_after_newline=False), Token(index=3, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=17, end_pos=19), first_token_after_newline=False), Token(index=4, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=20, end_pos=23), first_token_after_newline=False), Token(index=5, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=24, end_pos=31), first_token_after_newline=False), Token(index=6, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=32, end_pos=33), first_token_after_newline=False), Token(index=7, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=33, end_pos=36), first_token_after_newline=False), Token(index=8, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=37, end_pos=43), first_token_after_newline=False), Token(index=9, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=43, end_pos=45), first_token_after_newline=False), Token(index=10, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=46, end_pos=48), first_token_after_newline=False), Token(index=11, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=49, end_pos=56), first_token_after_newline=False), Token(index=12, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=57, end_pos=59), first_token_after_newline=False), Token(index=13, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=60, end_pos=61), first_token_after_newline=False), Token(index=14, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=62, end_pos=65), first_token_after_newline=False), Token(index=15, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=66, end_pos=68), first_token_after_newline=False), Token(index=16, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=69, end_pos=73), first_token_after_newline=False), Token(index=17, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=73, end_pos=74), first_token_after_newline=False), Token(index=18, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=75, end_pos=85), first_token_after_newline=False), Token(index=19, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=86, end_pos=88), first_token_after_newline=False), Token(index=20, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=89, end_pos=92), first_token_after_newline=False), Token(index=21, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=93, end_pos=97), first_token_after_newline=False), Token(index=22, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=98, end_pos=100), first_token_after_newline=False), Token(index=23, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=101, end_pos=103), first_token_after_newline=False), Token(index=24, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=104, end_pos=106), first_token_after_newline=False), Token(index=25, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=107, end_pos=113), first_token_after_newline=False), Token(index=26, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=113, end_pos=115), first_token_after_newline=False), Token(index=27, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=116, end_pos=121), first_token_after_newline=False), Token(index=28, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=122, end_pos=124), first_token_after_newline=False), Token(index=29, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=125, end_pos=129), first_token_after_newline=False), Token(index=30, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=130, end_pos=142), first_token_after_newline=False), Token(index=31, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=142, end_pos=143), first_token_after_newline=False), Token(index=32, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=144, end_pos=147), first_token_after_newline=False), Token(index=33, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=148, end_pos=150), first_token_after_newline=False), Token(index=34, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=151, end_pos=153), first_token_after_newline=False), Token(index=35, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=154, end_pos=158), first_token_after_newline=False), Token(index=36, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=159, end_pos=163), first_token_after_newline=False), Token(index=37, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=164, end_pos=166), first_token_after_newline=False), Token(index=38, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=167, end_pos=170), first_token_after_newline=False), Token(index=39, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=171, end_pos=176), first_token_after_newline=False), Token(index=40, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=176, end_pos=177), first_token_after_newline=False), Token(index=41, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=177, end_pos=178), first_token_after_newline=False), Token(index=42, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=179, end_pos=193), first_token_after_newline=False), Token(index=43, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=194, end_pos=197), first_token_after_newline=False), Token(index=44, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=198, end_pos=205), first_token_after_newline=False), Token(index=45, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=206, end_pos=209), first_token_after_newline=False), Token(index=46, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=210, end_pos=212), first_token_after_newline=False), Token(index=47, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=212, end_pos=213), first_token_after_newline=False), Token(index=48, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=213, end_pos=216), first_token_after_newline=False), Token(index=49, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=217, end_pos=220), first_token_after_newline=False), Token(index=50, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=220, end_pos=221), first_token_after_newline=False), Token(index=51, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=222, end_pos=227), first_token_after_newline=False), Token(index=52, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=228, end_pos=230), first_token_after_newline=False), Token(index=53, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=231, end_pos=240), first_token_after_newline=False), Token(index=54, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=241, end_pos=245), first_token_after_newline=False), Token(index=55, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=246, end_pos=250), first_token_after_newline=False), Token(index=56, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=251, end_pos=260), first_token_after_newline=False), Token(index=57, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=261, end_pos=265), first_token_after_newline=False), Token(index=58, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=266, end_pos=273), first_token_after_newline=False), Token(index=59, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=273, end_pos=274), first_token_after_newline=False), Token(index=60, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=274, end_pos=278), first_token_after_newline=False), Token(index=61, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=279, end_pos=283), first_token_after_newline=False), Token(index=62, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=283, end_pos=284), first_token_after_newline=False), Token(index=63, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=285, end_pos=290), first_token_after_newline=False), Token(index=64, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=291, end_pos=295), first_token_after_newline=False), Token(index=65, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=296, end_pos=298), first_token_after_newline=False), Token(index=66, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=299, end_pos=301), first_token_after_newline=False), Token(index=67, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=302, end_pos=308), first_token_after_newline=False), Token(index=68, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=309, end_pos=319), first_token_after_newline=False), Token(index=69, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=319, end_pos=320), first_token_after_newline=False), Token(index=70, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=320, end_pos=323), first_token_after_newline=False), Token(index=71, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=324, end_pos=331), first_token_after_newline=False), Token(index=72, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=331, end_pos=332), first_token_after_newline=False), Token(index=73, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=333, end_pos=334), first_token_after_newline=False), Token(index=74, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=335, end_pos=341), first_token_after_newline=False), Token(index=75, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=342, end_pos=347), first_token_after_newline=False), Token(index=76, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=348, end_pos=350), first_token_after_newline=False), Token(index=77, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=351, end_pos=355), first_token_after_newline=False), Token(index=78, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=356, end_pos=357), first_token_after_newline=False), Token(index=79, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=358, end_pos=361), first_token_after_newline=False), Token(index=80, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=362, end_pos=371), first_token_after_newline=False), Token(index=81, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=372, end_pos=374), first_token_after_newline=False), Token(index=82, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=375, end_pos=382), first_token_after_newline=False), Token(index=83, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=383, end_pos=386), first_token_after_newline=False), Token(index=84, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=387, end_pos=388), first_token_after_newline=False), Token(index=85, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=389, end_pos=393), first_token_after_newline=False), Token(index=86, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=393, end_pos=394), first_token_after_newline=False), Token(index=87, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=394, end_pos=395), first_token_after_newline=False), Token(index=88, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=396, end_pos=400), first_token_after_newline=False), Token(index=89, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=401, end_pos=403), first_token_after_newline=False), Token(index=90, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=404, end_pos=407), first_token_after_newline=False), Token(index=91, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=408, end_pos=409), first_token_after_newline=False), Token(index=92, token_type=<TokenType.NUMBER: 1>, char_interval=CharInterval(start_pos=409, end_pos=410), first_token_after_newline=False), Token(index=93, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=410, end_pos=412), first_token_after_newline=False), Token(index=94, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=413, end_pos=414), first_token_after_newline=False), Token(index=95, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=414, end_pos=415), first_token_after_newline=False), Token(index=96, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=416, end_pos=422), first_token_after_newline=False), Token(index=97, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=423, end_pos=425), first_token_after_newline=False), Token(index=98, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=425, end_pos=426), first_token_after_newline=False), Token(index=99, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=427, end_pos=431), first_token_after_newline=False), Token(index=100, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=432, end_pos=433), first_token_after_newline=False), Token(index=101, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=434, end_pos=440), first_token_after_newline=False), Token(index=102, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=441, end_pos=444), first_token_after_newline=False), Token(index=103, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=445, end_pos=447), first_token_after_newline=False), Token(index=104, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=448, end_pos=456), first_token_after_newline=False), Token(index=105, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=457, end_pos=465), first_token_after_newline=False), Token(index=106, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=466, end_pos=470), first_token_after_newline=False), Token(index=107, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=471, end_pos=473), first_token_after_newline=False), Token(index=108, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=473, end_pos=474), first_token_after_newline=False), Token(index=109, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=474, end_pos=476), first_token_after_newline=False), Token(index=110, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=476, end_pos=477), first_token_after_newline=False), Token(index=111, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=477, end_pos=481), first_token_after_newline=False), Token(index=112, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=482, end_pos=486), first_token_after_newline=False), Token(index=113, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=487, end_pos=489), first_token_after_newline=False), Token(index=114, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=490, end_pos=496), first_token_after_newline=False), Token(index=115, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=497, end_pos=505), first_token_after_newline=False), Token(index=116, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=505, end_pos=506), first_token_after_newline=False), Token(index=117, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=506, end_pos=507), first_token_after_newline=False), Token(index=118, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=508, end_pos=512), first_token_after_newline=False), Token(index=119, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=513, end_pos=517), first_token_after_newline=False), Token(index=120, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=518, end_pos=522), first_token_after_newline=False), Token(index=121, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=523, end_pos=528), first_token_after_newline=False), Token(index=122, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=529, end_pos=532), first_token_after_newline=False), Token(index=123, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=533, end_pos=537), first_token_after_newline=False), Token(index=124, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=538, end_pos=547), first_token_after_newline=False), Token(index=125, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=547, end_pos=548), first_token_after_newline=False), Token(index=126, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=549, end_pos=554), first_token_after_newline=False), Token(index=127, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=555, end_pos=557), first_token_after_newline=False), Token(index=128, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=558, end_pos=562), first_token_after_newline=False), Token(index=129, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=563, end_pos=564), first_token_after_newline=False), Token(index=130, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=565, end_pos=569), first_token_after_newline=False), Token(index=131, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=570, end_pos=572), first_token_after_newline=False), Token(index=132, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=573, end_pos=576), first_token_after_newline=False), Token(index=133, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=577, end_pos=578), first_token_after_newline=False), Token(index=134, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=579, end_pos=582), first_token_after_newline=False), Token(index=135, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=582, end_pos=583), first_token_after_newline=False)]), token_interval: TokenInterval(start_index=0, end_index=136)\n",
            "\u001b[94m\u001b[1mLangExtract\u001b[0m: model=\u001b[92mgemini-2.5-flash\u001b[0m, current=\u001b[92m583\u001b[0m chars, processed=\u001b[92m583\u001b[0m chars:  [00:00]DEBUG:absl:Processing chunk: TextChunk(\n",
            "  interval=[start_index: 0, end_index: 136],\n",
            "  Document ID: doc_273c46f8,\n",
            "  Chunk Text: 'Getting the same up and running (and stable!) in windows is a lot of work, especially if you want it to be smooth.. Linux is more configurable, and it is what most of the world's infrastructure and systems run on.And yes, Linux is generally much more efficient than Windows.That said, these days it is mainly preference.For example, I prefer Linux so that I can configure it exactly how I want.I like to use I3wm (A tiling wm) with a custom set of commands combined with oh-my-fish with my custom dotfiles.I have also used MacOS but only minimally, liked it when I used it for a bit.'\n",
            ")\n",
            "DEBUG:absl:Top inference result: {\"extractions\": [{\"distribution\": \"Windows\", \"distribution_attributes\": {\"User\": \"No\"}}, {\"distribution\": \"Linux\", \"distribution_attributes\": {\"User\": \"Yes\"}}, {\"distribution\": \"MacOS\", \"distribution_attributes\": {\"User\": \"Yes\"}}]}\n",
            "INFO:absl:Starting resolver process for input text.\n",
            "DEBUG:absl:Input Text: {\"extractions\": [{\"distribution\": \"Windows\", \"distribution_attributes\": {\"User\": \"No\"}}, {\"distribution\": \"Linux\", \"distribution_attributes\": {\"User\": \"Yes\"}}, {\"distribution\": \"MacOS\", \"distribution_attributes\": {\"User\": \"Yes\"}}]}\n",
            "INFO:absl:Starting string parsing.\n",
            "DEBUG:absl:input_string: {\"extractions\": [{\"distribution\": \"Windows\", \"distribution_attributes\": {\"User\": \"No\"}}, {\"distribution\": \"Linux\", \"distribution_attributes\": {\"User\": \"Yes\"}}, {\"distribution\": \"MacOS\", \"distribution_attributes\": {\"User\": \"Yes\"}}]}\n",
            "DEBUG:absl:Successfully parsed content.\n",
            "INFO:absl:Completed parsing of string.\n",
            "DEBUG:absl:Parsed content: [{'distribution': 'Windows', 'distribution_attributes': {'User': 'No'}}, {'distribution': 'Linux', 'distribution_attributes': {'User': 'Yes'}}, {'distribution': 'MacOS', 'distribution_attributes': {'User': 'Yes'}}]\n",
            "INFO:absl:Starting to extract and order extractions from data.\n",
            "INFO:absl:Completed extraction and ordering of extractions.\n",
            "DEBUG:absl:Completed the resolver process.\n",
            "INFO:absl:Starting alignment process for provided chunk text.\n",
            "DEBUG:absl:WordAligner: Starting alignment of extractions with the source text. Extraction groups to align: [[Extraction(extraction_class='distribution', extraction_text='Windows', char_interval=None, alignment_status=None, extraction_index=1, group_index=0, description=None, attributes={'User': 'No'}), Extraction(extraction_class='distribution', extraction_text='Linux', char_interval=None, alignment_status=None, extraction_index=2, group_index=1, description=None, attributes={'User': 'Yes'}), Extraction(extraction_class='distribution', extraction_text='MacOS', char_interval=None, alignment_status=None, extraction_index=3, group_index=2, description=None, attributes={'User': 'Yes'})]]\n",
            "2025-08-18 23:43:38,301 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text=\"Getting the same up and running (and stable!) in windows is a lot of work, especially if you want it to be smooth.. Linux is more configurable, and it is what most of the world's infrastructure and systems run on.And yes, Linux is generally much m...prefer Linux so that I can configure it exactly how I want.I like to use I3wm (A tiling wm) with a custom set of commands combined with oh-my-fish with my custom dotfiles.I have also used MacOS but only minimally, liked it when I used it for a bit.\")\n",
            "2025-08-18 23:43:38,307 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (4.1 ms)\n",
            "2025-08-18 23:43:38,309 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='␟')\n",
            "2025-08-18 23:43:38,310 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.0 ms)\n",
            "DEBUG:absl:Using delimiter '␟' for extraction alignment\n",
            "2025-08-18 23:43:38,311 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='Windows ␟ Linux ␟ MacOS')\n",
            "2025-08-18 23:43:38,311 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.1 ms)\n",
            "DEBUG:absl:Processing extraction group 0 with 3 extractions.\n",
            "2025-08-18 23:43:38,313 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='Windows')\n",
            "2025-08-18 23:43:38,313 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.0 ms)\n",
            "2025-08-18 23:43:38,316 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='Linux')\n",
            "2025-08-18 23:43:38,317 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.0 ms)\n",
            "2025-08-18 23:43:38,318 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='MacOS')\n",
            "2025-08-18 23:43:38,318 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.0 ms)\n",
            "2025-08-18 23:43:38,319 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text=\"Getting the same up and running (and stable!) in windows is a lot of work, especially if you want it to be smooth.. Linux is more configurable, and it is what most of the world's infrastructure and systems run on.And yes, Linux is generally much m...prefer Linux so that I can configure it exactly how I want.I like to use I3wm (A tiling wm) with a custom set of commands combined with oh-my-fish with my custom dotfiles.I have also used MacOS but only minimally, liked it when I used it for a bit.\")\n",
            "2025-08-18 23:43:38,321 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (1.0 ms)\n",
            "2025-08-18 23:43:38,327 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='Windows')\n",
            "2025-08-18 23:43:38,327 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.0 ms)\n",
            "2025-08-18 23:43:38,328 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='Linux')\n",
            "2025-08-18 23:43:38,329 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.0 ms)\n",
            "2025-08-18 23:43:38,329 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='MacOS')\n",
            "2025-08-18 23:43:38,330 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.0 ms)\n",
            "DEBUG:absl:Final aligned extraction groups: [[Extraction(extraction_class='distribution', extraction_text='Windows', char_interval=CharInterval(start_pos=49, end_pos=56), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=1, group_index=0, description=None, attributes={'User': 'No'}), Extraction(extraction_class='distribution', extraction_text='Linux', char_interval=CharInterval(start_pos=116, end_pos=121), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=2, group_index=1, description=None, attributes={'User': 'Yes'}), Extraction(extraction_class='distribution', extraction_text='MacOS', char_interval=CharInterval(start_pos=523, end_pos=528), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=3, group_index=2, description=None, attributes={'User': 'Yes'})]]\n",
            "DEBUG:absl:Aligned extractions count: 3\n",
            "DEBUG:absl:Yielding aligned extraction: Extraction(extraction_class='distribution', extraction_text='Windows', char_interval=CharInterval(start_pos=49, end_pos=56), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=1, group_index=0, description=None, attributes={'User': 'No'})\n",
            "DEBUG:absl:Yielding aligned extraction: Extraction(extraction_class='distribution', extraction_text='Linux', char_interval=CharInterval(start_pos=116, end_pos=121), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=2, group_index=1, description=None, attributes={'User': 'Yes'})\n",
            "DEBUG:absl:Yielding aligned extraction: Extraction(extraction_class='distribution', extraction_text='MacOS', char_interval=CharInterval(start_pos=523, end_pos=528), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=3, group_index=2, description=None, attributes={'User': 'Yes'})\n",
            "INFO:absl:Completed alignment process for the provided source_text.\n",
            "\u001b[94m\u001b[1mLangExtract\u001b[0m: model=\u001b[92mgemini-2.5-flash\u001b[0m, current=\u001b[92m583\u001b[0m chars, processed=\u001b[92m583\u001b[0m chars:  [00:01]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m✓\u001b[0m Extraction processing complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "INFO:absl:Finalizing annotation for document ID doc_273c46f8.\n",
            "INFO:absl:Document annotation completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m✓\u001b[0m Extracted \u001b[1m3\u001b[0m entities (\u001b[1m1\u001b[0m unique types)\n",
            "  \u001b[96m•\u001b[0m Time: \u001b[1m1.48s\u001b[0m\n",
            "  \u001b[96m•\u001b[0m Speed: \u001b[1m393\u001b[0m chars/sec\n",
            "  \u001b[96m•\u001b[0m Chunks: \u001b[1m1\u001b[0m\n",
            "Input: Getting the same up and running (and stable!) in windows is a lot of work, especially if you want it to be smooth.. Linux is more configurable, and it is what most of the world's infrastructure and systems run on.And yes, Linux is generally much more efficient than Windows.That said, these days it is mainly preference.For example, I prefer Linux so that I can configure it exactly how I want.I like to use I3wm (A tiling wm) with a custom set of commands combined with oh-my-fish with my custom dotfiles.I have also used MacOS but only minimally, liked it when I used it for a bit.\n",
            "\n",
            "Extracted entities:\n",
            "• Distribution: Windows (pos: 49-56)\n",
            "  Attributes: {'User': 'No'}\n",
            "• Distribution: Linux (pos: 116-121)\n",
            "  Attributes: {'User': 'Yes'}\n",
            "• Distribution: MacOS (pos: 523-528)\n",
            "  Attributes: {'User': 'Yes'}\n",
            "AnnotatedDocument(extractions=[Extraction(extraction_class='distribution', extraction_text='Windows', char_interval=CharInterval(start_pos=49, end_pos=56), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=1, group_index=0, description=None, attributes={'User': 'No'}), Extraction(extraction_class='distribution', extraction_text='Linux', char_interval=CharInterval(start_pos=116, end_pos=121), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=2, group_index=1, description=None, attributes={'User': 'Yes'}), Extraction(extraction_class='distribution', extraction_text='MacOS', char_interval=CharInterval(start_pos=523, end_pos=528), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=3, group_index=2, description=None, attributes={'User': 'Yes'})], text=\"Getting the same up and running (and stable!) in windows is a lot of work, especially if you want it to be smooth.. Linux is more configurable, and it is what most of the world's infrastructure and systems run on.And yes, Linux is generally much more efficient than Windows.That said, these days it is mainly preference.For example, I prefer Linux so that I can configure it exactly how I want.I like to use I3wm (A tiling wm) with a custom set of commands combined with oh-my-fish with my custom dotfiles.I have also used MacOS but only minimally, liked it when I used it for a bit.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[94m\u001b[1mLangExtract\u001b[0m: Saving to \u001b[92mlinux_extract.jsonl\u001b[0m: 1 docs [00:00, 15.08 docs/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m✓\u001b[0m Saved \u001b[1m1\u001b[0m documents to \u001b[92mlinux_extract.jsonl\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[94m\u001b[1mLangExtract\u001b[0m: Loading \u001b[92mlinux_extract.jsonl\u001b[0m: 100%|██████████| 1.38k/1.38k [00:00<00:00, 485kB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m✓\u001b[0m Loaded \u001b[1m1\u001b[0m documents from \u001b[92mlinux_extract.jsonl\u001b[0m\n",
            "Interactive visualization saved to linux_extractvisualization.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Previous system prompt\n",
        "system_prompt_old = \"Extract operating system being used by the post author\"\n",
        "\n",
        "#Updated system prompt\n",
        "system_prompt_new = \"Extract operating system mentioned by the post author. Make sure to provide all instances of operating systems mentioned in the post.\" \\\n",
        "\"Each post might contain mention of multiple operating systems, extract them all including multple instances of the same operating system.\""
      ],
      "metadata": {
        "id": "BqtoHRZbCQyl"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}